{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nnfs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNsxkjwECzpqcKlhe3S2gaT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chosh84/ml_study/blob/main/NNFS/notebooks/nnfs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag5xegBdoj0z",
        "outputId": "291a56c3-8afd-4ccc-c95d-5c9f742c33c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nnfs in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nnfs) (1.21.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "\n",
        "nnfs.init()"
      ],
      "metadata": {
        "id": "SzaKjxOjkNLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# neuron class 생성\n",
        "\n",
        "class Layer_Dense:\n",
        "\n",
        "  # Layer initialization\n",
        "  def __init__(self, n_inputs, n_neurons,\n",
        "               weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
        "               bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
        "    #Initialize weights and biases\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "    # Set regularization strength\n",
        "    self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "    self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "    self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "    self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # Remembering inputs for calculating backward pass of weights\n",
        "    self.inputs = inputs\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "  \n",
        "  #backward pass\n",
        "  def backward(self, dvalues):\n",
        "    # Gradients on parameters\n",
        "    self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "    self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "    \n",
        "    # Gradients on regularization\n",
        "    # L1 on weights\n",
        "    if self.weight_regularizer_l1 > 0:\n",
        "      dL1 = np.ones_like(self.weights)\n",
        "      dL1[self.weights < 0] = -1\n",
        "      self.dweights += self.weight_regularizer_l1 * dL1\n",
        "    \n",
        "    # L1 on biases\n",
        "    if self.bias_regularizer_l1 > 0:\n",
        "      dL1 = np.ones_like(self.biases)\n",
        "      dL1[self.biases < 0] = -1\n",
        "      self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "\n",
        "    # L2 on weights\n",
        "    if self.weight_regularizer_l2 > 0:\n",
        "      self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
        "    \n",
        "    # L2 on biases\n",
        "    if self.bias_regularizer_l2 > 0:\n",
        "      self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
        "    \n",
        "    # Gradients on values\n",
        "    self.dinputs = np.dot(dvalues, self.weights.T)"
      ],
      "metadata": {
        "id": "6jcspCUukRZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropout\n",
        "class Layer_Dropout:\n",
        "  # Init\n",
        "  def __init__(self, rate):\n",
        "    # Store rate, we invert it as for example for dropout \n",
        "    # of 0.1 we need success rate of 0.9\n",
        "    self.rate = 1 - rate\n",
        "  \n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Save input values\n",
        "    self.inputs = inputs\n",
        "    # Generate and save scaled mask\n",
        "    self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
        "    # Apply mask to output values\n",
        "    self.output = inputs * self.binary_mask\n",
        "  \n",
        "  # Backward pass\n",
        "  def backward(self, dvalues):\n",
        "    # Gradient on values\n",
        "    self.dinputs = dvalues * self.binary_mask"
      ],
      "metadata": {
        "id": "yWhaoczJFlqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU Activiation class 생성\n",
        "\n",
        "class Activation_ReLU:\n",
        "\n",
        "  #Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Remembering inputs for calculating backward pass of weights\n",
        "    self.inputs = inputs\n",
        "    # Calculate output values from input\n",
        "    self.output = np.maximum(0, inputs)\n",
        "\n",
        "  # Backward pass\n",
        "  def backward(self, dvalues):\n",
        "    # Since we need to modify the original variables,\n",
        "    # let's make a copy of the values first\n",
        "    self.dinputs = dvalues.copy()\n",
        "\n",
        "    # Zero gradient where input values are negetive\n",
        "    self.dinputs[self.inputs <= 0] = 0"
      ],
      "metadata": {
        "id": "QMw_kvyRScmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Activation class 생성\n",
        "# output 간의 확율로 변환 (총합이 1인 확율로 변환)\n",
        "\n",
        "class Activation_Softmax:\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Get unnormalized probabilities\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "\n",
        "    # Normalize them for each sample\n",
        "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "\n",
        "    self.output =probabilities\n",
        "\n",
        "  # Backward pass\n",
        "  def backward(self, dvalues):\n",
        "    # Create uninitialized array\n",
        "    self.dinputs = np.empty_like(dvalues)\n",
        "    # Enumerate outputs and gradients\n",
        "    for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
        "      # Flatten output array\n",
        "      single_output = single_output.reshape(-1, 1)\n",
        "      # Calculate Jacobian matrix of the output\n",
        "      jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "      # Calculate sample-wise gradient\n",
        "      # and add it to the array of sample gradients\n",
        "      self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
        "\n"
      ],
      "metadata": {
        "id": "kqjvNmuTp3eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Common loss class\n",
        "class Loss:\n",
        "\n",
        "  # Calulates the data and regularization losses\n",
        "  # given model output and ground truth values\n",
        "  def calculate(self, output, y):\n",
        "\n",
        "    # Calculate sample losses\n",
        "    sample_losses = self.forward(output, y)\n",
        "\n",
        "    # Calculate mean loss\n",
        "    data_loss = np.mean(sample_losses)\n",
        "\n",
        "    # Return loss\n",
        "    return data_loss\n",
        "\n",
        "  # Regularization loss calculation\n",
        "  def regularization_loss(self, layer):\n",
        "    # 0 by default\n",
        "    regularization_loss = 0\n",
        "\n",
        "    # L1 regularization - weights\n",
        "    # calculate only when factor greater than 0\n",
        "    if layer.weight_regularizer_l1 > 0:\n",
        "      regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
        "    \n",
        "    # L2 regularization - weights\n",
        "    if layer.weight_regularizer_l2 > 0:\n",
        "      regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
        "\n",
        "    # L1 regularization - bias\n",
        "    # calculate only when factor greater than 0\n",
        "    if layer.bias_regularizer_l1 > 0:\n",
        "      regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
        "    \n",
        "    # L2 regularization - bias\n",
        "    if layer.bias_regularizer_l2 > 0:\n",
        "      regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
        "\n",
        "    return regularization_loss"
      ],
      "metadata": {
        "id": "m0ohO1sH2oe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-entropy loss\n",
        "\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, y_pred, y_true):\n",
        "\n",
        "    # Number of samples in a batch\n",
        "    samples = len(y_pred)\n",
        "\n",
        "    # Clip data to prevent division by 0\n",
        "    # Clip both sides to not drag mean towards any value\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    # Probabilities of target values\n",
        "    # only if categorical labels\n",
        "    if len(y_true.shape) == 1:\n",
        "      correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "    # Mask values - only for one-hot encoded labels\n",
        "    elif len(y_true.shape) == 2:\n",
        "      correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
        "\n",
        "    # Losses\n",
        "    negative_log_likelihoods = -np.log(correct_confidences)\n",
        "    return negative_log_likelihoods\n",
        "\n",
        "  # Backward pass\n",
        "  def backward(self, dvalues, y_true):\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "    # Number of lables in every sample\n",
        "    # We'll use the first sample to count them\n",
        "    labels = len(dvalues[0])\n",
        "    # If lables are sparse, turn them into one-hot vector\n",
        "    if len(y_true.shape) == 1:\n",
        "      y_true = np.eye(labels)[y_true]\n",
        "    # Calculate gradient\n",
        "    self.dinputs = - y_true /dvalues\n",
        "    # Normalize gradient\n",
        "    self.dinputs = self.dinputs / samples\n",
        "    print(self.dinputs)"
      ],
      "metadata": {
        "id": "J41Squ1F3RGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "  # Creates activation and loss function objects\n",
        "  def __init__(self):\n",
        "    self.activation = Activation_Softmax()\n",
        "    self.loss = Loss_CategoricalCrossentropy()\n",
        "  \n",
        "  # Forward pass\n",
        "  def forward(self, inputs, y_true):\n",
        "    # Output layer's activation function\n",
        "    self.activation.forward(inputs)\n",
        "    # Set the output\n",
        "    self.output = self.activation.output\n",
        "    # Calculate and return loss value\n",
        "    return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "  # Backward pass\n",
        "  def backward(self, dvalues, y_true):\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "    # If labels are one-hot encoded,\n",
        "    # turn them in to discrete values\n",
        "    if len(y_true.shape)==2:\n",
        "      y_true = np.argmax(y_true, axis=1)\n",
        "    # Copy so we can safely modify\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Calculate gradient\n",
        "    self.dinputs[range(samples),y_true] -= 1\n",
        "    # Normalize gradient\n",
        "    self.dinputs = self.dinputs / samples\n",
        "    "
      ],
      "metadata": {
        "id": "UlUtg3G6bGtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD optimizer\n",
        "class Optimizer_SGD:\n",
        "\n",
        "  # Initialize optimizer - set settings, \n",
        "  # learning rate of 1. is default for this optimizer\n",
        "  def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.momentum = momentum\n",
        "  \n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "  # Update parameters\n",
        "  def update_params(self, layer):\n",
        "    \n",
        "    # If we use momentum\n",
        "    if self.momentum:\n",
        "      # If layer does not contain momentum arrays, create them\n",
        "      # filled with zeros\n",
        "      if not hasattr(layer, 'weight_momentums'):\n",
        "        layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "        # If there is no momentum array for weights\n",
        "        # The array doesn't exists for biases yet either\n",
        "        layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "      \n",
        "      # Build weight updates  with momentum - take previous\n",
        "      # update multiplied by retain factor and update with \n",
        "      # current gradients\n",
        "      weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
        "      layer.weight_momentums = weight_updates\n",
        "\n",
        "      # Build biases updates\n",
        "      bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
        "      layer.bias_momentums = bias_updates\n",
        "\n",
        "    # Vanilla SGD updates (as before momentum update)\n",
        "    else:\n",
        "      weight_updates = -self.current_learning_rate * layer.dweights\n",
        "      bias_updates = -self.current_learning_rate * layer.dbiases\n",
        "\n",
        "    # Update weights and biases using either\n",
        "    # vanilla or momentum updates\n",
        "    layer.weights += weight_updates\n",
        "    layer.biases += bias_updates\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1"
      ],
      "metadata": {
        "id": "pQsCrT3n134x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adagrad optimizer\n",
        "class Optimizer_Adagrad:\n",
        "\n",
        "  # Initialize optimizer - set settings, \n",
        "  def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "  \n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "  # Update parameters\n",
        "  def update_params(self, layer):\n",
        "    # If layer does not contain cache arrays, create them\n",
        "    # filled with zeros\n",
        "    if not hasattr(layer, 'weight_cache'):\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "    \n",
        "    # Update cach with squared current gradients\n",
        "    layer.weight_cache += layer.dweights**2\n",
        "    layer.bias_cache += layer.dbiases**2\n",
        "\n",
        "    # Vanilla SGD parameter update + normalization \n",
        "    # with square rooted cache\n",
        "    layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "    layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1"
      ],
      "metadata": {
        "id": "b5J2TjqVjPBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RMSprop optimizer\n",
        "class Optimizer_RMSprop:\n",
        "  # Initialize optimizer - set settings\n",
        "  def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, rho=0.9):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.rho = rho\n",
        "  \n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params(self): \n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "  \n",
        "  # Update parameters\n",
        "  def update_params(self, layer):\n",
        "    # If layer does not contain cache arrays,\n",
        "    # create them filled with zeros\n",
        "    if not hasattr(layer, 'weight_cache'):\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "    \n",
        "    # Update cache with squared current gradients\n",
        "    layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
        "    layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
        "\n",
        "    # Vanilla SGD parameter update + normalization \n",
        "    # with square rooted cache\n",
        "    layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "    layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "  \n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1"
      ],
      "metadata": {
        "id": "R8ymIskE5dZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam optimizer\n",
        "class Optimizer_Adam:\n",
        "  # Initialize optimizer - set settings\n",
        "  def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "  \n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params(self): \n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "  \n",
        "  # Update parameters\n",
        "  def update_params(self, layer):\n",
        "    # If layer does not contain cache arrays,\n",
        "    # create them filled with zeros\n",
        "    if not hasattr(layer, 'weight_cache'):\n",
        "      layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "    \n",
        "    # Update momentum with current gradients\n",
        "    layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
        "    layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
        "\n",
        "    # Get corrected momentum \n",
        "    # self.iteration is 0 at first pass\n",
        "    # and we need to start with 1 here\n",
        "    weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "    bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "\n",
        "    # Update cache with the squared current gradients\n",
        "    layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
        "    layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
        "\n",
        "    # Get corrected cache\n",
        "    weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "    bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "    \n",
        "    # Vanilla SGD parameter update + normalization \n",
        "    # with square rooted cache\n",
        "    layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "    layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "    \n",
        "  \n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1"
      ],
      "metadata": {
        "id": "vqr2V0mxC_zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2차코드\n",
        "#dataset 가지고 오기\n",
        "X, y = spiral_data(samples=1000, classes=3)\n",
        "\n",
        "# Create Dense layer with 2 input features and 64 output values\n",
        "#dense1 = Layer_Dense(2,64, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
        "dense1 = Layer_Dense(2,512, weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4)\n",
        "\n",
        "# Create dropout layer\n",
        "dropout1 = Layer_Dropout(0.1)\n",
        "\n",
        "# ReLU Activation 만들기\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create secon Dense layer with 64 input features (as we take output of previous layer here) and 3 output values\n",
        "#dense2 = Layer_Dense(64,3)\n",
        "dense2 = Layer_Dense(512,3)\n",
        "\n",
        "# Create loss function\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Create optimizer\n",
        "#optimizer = Optimizer_SGD(decay=1e-3, momentum=0.9)\n",
        "#optimizer = Optimizer_Adagrad(decay=1e-4)\n",
        "#optimizer = Optimizer_RMSprop(learning_rate=0.02, decay=1e-5, rho=0.999)\n",
        "optimizer = Optimizer_Adam(learning_rate=0.05, decay=5e-5)\n",
        "\n",
        "# Train in loop\n",
        "for epoch in range(10001):\n",
        "  # Make a forward pass of our training data through this layer\n",
        "  dense1.forward(X)\n",
        "\n",
        "  # Forward pass through activation function\n",
        "  # takes in output from previous layer\n",
        "  activation1.forward(dense1.output)\n",
        "\n",
        "  # Perform a forward pass through Dropout layer\n",
        "  dropout1.forward(activation1.output)\n",
        "  \n",
        "  # Perform a forward pass through second Dense layer\n",
        "  # takes outputs of activation function of first layer as inputs\n",
        "  dense2.forward(dropout1.output)# Perform a forward pass through Dropout layer\n",
        "\n",
        "  # Make a forward pass through second Dense layer\n",
        "  # it takes output of activation function of first layer as intputs\n",
        "  #dense2.forward(activation1.output)\n",
        "\n",
        "  # Calculate loss from output of activation2 so softmax activation\n",
        "  data_loss = loss_activation.forward(dense2.output, y)\n",
        "\n",
        "  # Calculate regularization penalty\n",
        "  regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2)\n",
        "\n",
        "  # Calculate overall loss\n",
        "  loss = data_loss + regularization_loss\n",
        "\n",
        "  # Calculate accuracy from output of activation2 and targets\n",
        "  # calculate values along the first axis\n",
        "  predictions = np.argmax(loss_activation.output, axis=1)\n",
        "  if len(y.shape) == 2:\n",
        "    y = np.argmax(y, axis=1)\n",
        "  accuracy = np.mean(predictions == y)\n",
        "\n",
        "  # Print accuracy\n",
        "  if not epoch % 100:\n",
        "    print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f},  ' + \n",
        "          f'loss: {loss:.3f}, (' + \n",
        "          f'data_loss: {data_loss:.3f}, ' + \n",
        "          f'reg_loss: {regularization_loss:.3f}), ' + \n",
        "          f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "  # Backward pass\n",
        "  loss_activation.backward(loss_activation.output, y)\n",
        "  dense2.backward(loss_activation.dinputs)\n",
        "  dropout1.backward(dense2.dinputs)\n",
        "  activation1.backward(dropout1.dinputs)\n",
        "  #activation1.backward(dense2.dinputs)\n",
        "  dense1.backward(activation1.dinputs)\n",
        "\n",
        "  # Update weights and biases\n",
        "  optimizer.pre_update_params()\n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)\n",
        "  optimizer.post_update_params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OzNQoA44PzZ",
        "outputId": "18351652-e1b8-4602-ee3c-aad6b0474293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.373,  loss: 1.099, (data_loss: 1.099, reg_loss: 0.000), lr: 0.05\n",
            "epoch: 100, acc: 0.715,  loss: 0.733, (data_loss: 0.672, reg_loss: 0.062), lr: 0.04975371909050202\n",
            "epoch: 200, acc: 0.783,  loss: 0.621, (data_loss: 0.542, reg_loss: 0.079), lr: 0.049507401356502806\n",
            "epoch: 300, acc: 0.813,  loss: 0.582, (data_loss: 0.501, reg_loss: 0.081), lr: 0.0492635105177595\n",
            "epoch: 400, acc: 0.821,  loss: 0.568, (data_loss: 0.488, reg_loss: 0.080), lr: 0.04902201088288642\n",
            "epoch: 500, acc: 0.832,  loss: 0.536, (data_loss: 0.458, reg_loss: 0.078), lr: 0.048782867456949125\n",
            "epoch: 600, acc: 0.831,  loss: 0.527, (data_loss: 0.449, reg_loss: 0.078), lr: 0.04854604592455945\n",
            "epoch: 700, acc: 0.805,  loss: 0.537, (data_loss: 0.463, reg_loss: 0.074), lr: 0.048311512633460556\n",
            "epoch: 800, acc: 0.835,  loss: 0.497, (data_loss: 0.424, reg_loss: 0.073), lr: 0.04807923457858551\n",
            "epoch: 900, acc: 0.842,  loss: 0.477, (data_loss: 0.405, reg_loss: 0.072), lr: 0.04784917938657352\n",
            "epoch: 1000, acc: 0.820,  loss: 0.509, (data_loss: 0.437, reg_loss: 0.072), lr: 0.04762131530072861\n",
            "epoch: 1100, acc: 0.850,  loss: 0.483, (data_loss: 0.413, reg_loss: 0.070), lr: 0.04739561116640599\n",
            "epoch: 1200, acc: 0.848,  loss: 0.464, (data_loss: 0.396, reg_loss: 0.068), lr: 0.04717203641681212\n",
            "epoch: 1300, acc: 0.850,  loss: 0.462, (data_loss: 0.396, reg_loss: 0.067), lr: 0.04695056105920466\n",
            "epoch: 1400, acc: 0.843,  loss: 0.528, (data_loss: 0.462, reg_loss: 0.066), lr: 0.04673115566147951\n",
            "epoch: 1500, acc: 0.829,  loss: 0.470, (data_loss: 0.404, reg_loss: 0.066), lr: 0.046513791339132055\n",
            "epoch: 1600, acc: 0.860,  loss: 0.450, (data_loss: 0.377, reg_loss: 0.073), lr: 0.04629843974258068\n",
            "epoch: 1700, acc: 0.847,  loss: 0.480, (data_loss: 0.413, reg_loss: 0.068), lr: 0.046085073044840774\n",
            "epoch: 1800, acc: 0.855,  loss: 0.462, (data_loss: 0.398, reg_loss: 0.064), lr: 0.04587366392953806\n",
            "epoch: 1900, acc: 0.859,  loss: 0.461, (data_loss: 0.398, reg_loss: 0.063), lr: 0.04566418557925019\n",
            "epoch: 2000, acc: 0.860,  loss: 0.476, (data_loss: 0.413, reg_loss: 0.062), lr: 0.045456611664166556\n",
            "epoch: 2100, acc: 0.861,  loss: 0.440, (data_loss: 0.378, reg_loss: 0.062), lr: 0.045250916331055706\n",
            "epoch: 2200, acc: 0.867,  loss: 0.437, (data_loss: 0.375, reg_loss: 0.063), lr: 0.0450470741925312\n",
            "epoch: 2300, acc: 0.855,  loss: 0.450, (data_loss: 0.390, reg_loss: 0.061), lr: 0.04484506031660612\n",
            "epoch: 2400, acc: 0.858,  loss: 0.442, (data_loss: 0.382, reg_loss: 0.060), lr: 0.04464485021652753\n",
            "epoch: 2500, acc: 0.863,  loss: 0.442, (data_loss: 0.383, reg_loss: 0.060), lr: 0.044446419840881816\n",
            "epoch: 2600, acc: 0.842,  loss: 0.462, (data_loss: 0.402, reg_loss: 0.060), lr: 0.04424974556396301\n",
            "epoch: 2700, acc: 0.864,  loss: 0.440, (data_loss: 0.381, reg_loss: 0.059), lr: 0.04405480417639544\n",
            "epoch: 2800, acc: 0.867,  loss: 0.435, (data_loss: 0.377, reg_loss: 0.059), lr: 0.04386157287600334\n",
            "epoch: 2900, acc: 0.864,  loss: 0.434, (data_loss: 0.374, reg_loss: 0.060), lr: 0.04367002925891961\n",
            "epoch: 3000, acc: 0.869,  loss: 0.431, (data_loss: 0.372, reg_loss: 0.059), lr: 0.043480151310926564\n",
            "epoch: 3100, acc: 0.857,  loss: 0.432, (data_loss: 0.373, reg_loss: 0.059), lr: 0.04329191739902161\n",
            "epoch: 3200, acc: 0.872,  loss: 0.422, (data_loss: 0.365, reg_loss: 0.057), lr: 0.043105306263201\n",
            "epoch: 3300, acc: 0.863,  loss: 0.411, (data_loss: 0.353, reg_loss: 0.058), lr: 0.0429202970084553\n",
            "epoch: 3400, acc: 0.848,  loss: 0.454, (data_loss: 0.395, reg_loss: 0.059), lr: 0.04273686909696996\n",
            "epoch: 3500, acc: 0.860,  loss: 0.439, (data_loss: 0.382, reg_loss: 0.057), lr: 0.04255500234052514\n",
            "epoch: 3600, acc: 0.848,  loss: 0.458, (data_loss: 0.402, reg_loss: 0.057), lr: 0.042374676893088686\n",
            "epoch: 3700, acc: 0.876,  loss: 0.425, (data_loss: 0.368, reg_loss: 0.057), lr: 0.042195873243596776\n",
            "epoch: 3800, acc: 0.859,  loss: 0.427, (data_loss: 0.371, reg_loss: 0.056), lr: 0.04201857220891634\n",
            "epoch: 3900, acc: 0.854,  loss: 0.485, (data_loss: 0.427, reg_loss: 0.058), lr: 0.041842754926984395\n",
            "epoch: 4000, acc: 0.878,  loss: 0.418, (data_loss: 0.359, reg_loss: 0.059), lr: 0.04166840285011875\n",
            "epoch: 4100, acc: 0.870,  loss: 0.420, (data_loss: 0.364, reg_loss: 0.056), lr: 0.041495497738495375\n",
            "epoch: 4200, acc: 0.875,  loss: 0.413, (data_loss: 0.358, reg_loss: 0.055), lr: 0.041324021653787346\n",
            "epoch: 4300, acc: 0.872,  loss: 0.420, (data_loss: 0.366, reg_loss: 0.055), lr: 0.041153956952961035\n",
            "epoch: 4400, acc: 0.851,  loss: 0.501, (data_loss: 0.440, reg_loss: 0.061), lr: 0.040985286282224684\n",
            "epoch: 4500, acc: 0.871,  loss: 0.413, (data_loss: 0.352, reg_loss: 0.061), lr: 0.04081799257112535\n",
            "epoch: 4600, acc: 0.864,  loss: 0.440, (data_loss: 0.382, reg_loss: 0.058), lr: 0.04065205902678971\n",
            "epoch: 4700, acc: 0.875,  loss: 0.412, (data_loss: 0.357, reg_loss: 0.055), lr: 0.04048746912830479\n",
            "epoch: 4800, acc: 0.865,  loss: 0.419, (data_loss: 0.364, reg_loss: 0.055), lr: 0.04032420662123473\n",
            "epoch: 4900, acc: 0.871,  loss: 0.416, (data_loss: 0.363, reg_loss: 0.053), lr: 0.04016225551226957\n",
            "epoch: 5000, acc: 0.875,  loss: 0.419, (data_loss: 0.364, reg_loss: 0.055), lr: 0.04000160006400256\n",
            "epoch: 5100, acc: 0.871,  loss: 0.421, (data_loss: 0.367, reg_loss: 0.053), lr: 0.039842224789832265\n",
            "epoch: 5200, acc: 0.875,  loss: 0.400, (data_loss: 0.348, reg_loss: 0.053), lr: 0.03968411444898608\n",
            "epoch: 5300, acc: 0.862,  loss: 0.441, (data_loss: 0.389, reg_loss: 0.052), lr: 0.03952725404166173\n",
            "epoch: 5400, acc: 0.842,  loss: 0.482, (data_loss: 0.429, reg_loss: 0.053), lr: 0.03937162880428363\n",
            "epoch: 5500, acc: 0.857,  loss: 0.413, (data_loss: 0.360, reg_loss: 0.053), lr: 0.03921722420487078\n",
            "epoch: 5600, acc: 0.870,  loss: 0.417, (data_loss: 0.366, reg_loss: 0.051), lr: 0.03906402593851323\n",
            "epoch: 5700, acc: 0.878,  loss: 0.402, (data_loss: 0.350, reg_loss: 0.051), lr: 0.038912019922954205\n",
            "epoch: 5800, acc: 0.883,  loss: 0.401, (data_loss: 0.348, reg_loss: 0.053), lr: 0.038761192294274965\n",
            "epoch: 5900, acc: 0.865,  loss: 0.425, (data_loss: 0.373, reg_loss: 0.052), lr: 0.038611529402679645\n",
            "epoch: 6000, acc: 0.868,  loss: 0.419, (data_loss: 0.369, reg_loss: 0.050), lr: 0.03846301780837725\n",
            "epoch: 6100, acc: 0.870,  loss: 0.406, (data_loss: 0.354, reg_loss: 0.052), lr: 0.03831564427755853\n",
            "epoch: 6200, acc: 0.866,  loss: 0.425, (data_loss: 0.373, reg_loss: 0.052), lr: 0.03816939577846483\n",
            "epoch: 6300, acc: 0.872,  loss: 0.427, (data_loss: 0.369, reg_loss: 0.057), lr: 0.038024259477546674\n",
            "epoch: 6400, acc: 0.881,  loss: 0.393, (data_loss: 0.338, reg_loss: 0.055), lr: 0.03788022273570969\n",
            "epoch: 6500, acc: 0.873,  loss: 0.410, (data_loss: 0.357, reg_loss: 0.053), lr: 0.03773727310464546\n",
            "epoch: 6600, acc: 0.874,  loss: 0.419, (data_loss: 0.367, reg_loss: 0.051), lr: 0.03759539832324524\n",
            "epoch: 6700, acc: 0.867,  loss: 0.403, (data_loss: 0.352, reg_loss: 0.051), lr: 0.03745458631409416\n",
            "epoch: 6800, acc: 0.867,  loss: 0.435, (data_loss: 0.385, reg_loss: 0.049), lr: 0.03731482518004403\n",
            "epoch: 6900, acc: 0.873,  loss: 0.403, (data_loss: 0.354, reg_loss: 0.050), lr: 0.03717610320086248\n",
            "epoch: 7000, acc: 0.879,  loss: 0.406, (data_loss: 0.356, reg_loss: 0.050), lr: 0.03703840882995667\n",
            "epoch: 7100, acc: 0.867,  loss: 0.444, (data_loss: 0.393, reg_loss: 0.051), lr: 0.036901730691169414\n",
            "epoch: 7200, acc: 0.868,  loss: 0.429, (data_loss: 0.379, reg_loss: 0.050), lr: 0.03676605757564617\n",
            "epoch: 7300, acc: 0.870,  loss: 0.443, (data_loss: 0.392, reg_loss: 0.051), lr: 0.03663137843877066\n",
            "epoch: 7400, acc: 0.864,  loss: 0.421, (data_loss: 0.370, reg_loss: 0.051), lr: 0.03649768239716778\n",
            "epoch: 7500, acc: 0.871,  loss: 0.414, (data_loss: 0.364, reg_loss: 0.051), lr: 0.03636495872577185\n",
            "epoch: 7600, acc: 0.869,  loss: 0.414, (data_loss: 0.364, reg_loss: 0.051), lr: 0.03623319685495851\n",
            "epoch: 7700, acc: 0.881,  loss: 0.413, (data_loss: 0.363, reg_loss: 0.051), lr: 0.03610238636773891\n",
            "epoch: 7800, acc: 0.872,  loss: 0.408, (data_loss: 0.358, reg_loss: 0.050), lr: 0.03597251699701428\n",
            "epoch: 7900, acc: 0.867,  loss: 0.444, (data_loss: 0.395, reg_loss: 0.050), lr: 0.035843578622889706\n",
            "epoch: 8000, acc: 0.645,  loss: 0.827, (data_loss: 0.739, reg_loss: 0.088), lr: 0.03571556127004536\n",
            "epoch: 8100, acc: 0.799,  loss: 0.610, (data_loss: 0.513, reg_loss: 0.097), lr: 0.03558845510516389\n",
            "epoch: 8200, acc: 0.785,  loss: 0.584, (data_loss: 0.493, reg_loss: 0.092), lr: 0.03546225043441257\n",
            "epoch: 8300, acc: 0.811,  loss: 0.555, (data_loss: 0.468, reg_loss: 0.086), lr: 0.035336937700978836\n",
            "epoch: 8400, acc: 0.813,  loss: 0.559, (data_loss: 0.477, reg_loss: 0.081), lr: 0.03521250748265784\n",
            "epoch: 8500, acc: 0.820,  loss: 0.543, (data_loss: 0.465, reg_loss: 0.078), lr: 0.035088950489490865\n",
            "epoch: 8600, acc: 0.818,  loss: 0.530, (data_loss: 0.457, reg_loss: 0.073), lr: 0.0349662575614532\n",
            "epoch: 8700, acc: 0.832,  loss: 0.509, (data_loss: 0.439, reg_loss: 0.070), lr: 0.034844419666190465\n",
            "epoch: 8800, acc: 0.838,  loss: 0.518, (data_loss: 0.450, reg_loss: 0.069), lr: 0.034723427896801974\n",
            "epoch: 8900, acc: 0.824,  loss: 0.516, (data_loss: 0.451, reg_loss: 0.065), lr: 0.03460327346967023\n",
            "epoch: 9000, acc: 0.836,  loss: 0.501, (data_loss: 0.437, reg_loss: 0.063), lr: 0.034483947722335255\n",
            "epoch: 9100, acc: 0.832,  loss: 0.523, (data_loss: 0.462, reg_loss: 0.061), lr: 0.034365442111412764\n",
            "epoch: 9200, acc: 0.844,  loss: 0.493, (data_loss: 0.434, reg_loss: 0.059), lr: 0.03424774821055516\n",
            "epoch: 9300, acc: 0.827,  loss: 0.483, (data_loss: 0.425, reg_loss: 0.057), lr: 0.03413085770845422\n",
            "epoch: 9400, acc: 0.838,  loss: 0.498, (data_loss: 0.442, reg_loss: 0.055), lr: 0.034014762406884586\n",
            "epoch: 9500, acc: 0.842,  loss: 0.486, (data_loss: 0.432, reg_loss: 0.054), lr: 0.03389945421878708\n",
            "epoch: 9600, acc: 0.841,  loss: 0.478, (data_loss: 0.426, reg_loss: 0.053), lr: 0.033784925166390756\n",
            "epoch: 9700, acc: 0.845,  loss: 0.471, (data_loss: 0.419, reg_loss: 0.052), lr: 0.03367116737937304\n",
            "epoch: 9800, acc: 0.838,  loss: 0.481, (data_loss: 0.430, reg_loss: 0.051), lr: 0.033558173093056816\n",
            "epoch: 9900, acc: 0.847,  loss: 0.478, (data_loss: 0.428, reg_loss: 0.050), lr: 0.0334459346466437\n",
            "epoch: 10000, acc: 0.849,  loss: 0.476, (data_loss: 0.427, reg_loss: 0.050), lr: 0.03333444448148271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate the model\n",
        "\n",
        "# Create test dataset\n",
        "X_test, y_test = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Perform a forward pass of our testing data through this layer \n",
        "dense1.forward(X_test)\n",
        "# Perform a forward pass through activation function\n",
        "# takes the output of first dense layer here\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "# Perform a forward pass through second Dense layer\n",
        "# takes outputs of activation function of first layer as inputs\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "# Perform a forward pass through the activation/loss function\n",
        "# takes the output of second dense layer here and returns loss\n",
        "loss = loss_activation.forward(dense2.output, y_test)\n",
        "\n",
        "# Calculate accuracy from output of activation2 and targets # calculate values along first axis\n",
        "predictions = np.argmax(loss_activation.output, axis=1)\n",
        "\n",
        "if len(y_test.shape) == 2:\n",
        "  y_test = np.argmax(y_test, axis=1)\n",
        "accuracy = np.mean(predictions == y_test)\n",
        "\n",
        "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}')\n"
      ],
      "metadata": {
        "id": "THChgZAnZp_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac196e22-9cea-4f58-fa16-4d4b1aed097d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "validation, acc: 0.857, loss: 0.399\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "# 기존코드\n",
        "#dataset 가지고 오기\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# Dense layer 만들기 2 input features and 3 output values\n",
        "dense1 = Layer_Dense(2,3)\n",
        "\n",
        "# ReLU Activation 만들기\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense Layer with 3 input features (as we take output of previous layer here) and 3 output values\n",
        "dense2 = Layer_Dense(3,3)\n",
        "\n",
        "# Create Softmax activation (to be used with Dense Layer)\n",
        "#activation2 = Activation_Softmax()\n",
        "\n",
        "# Create loss function\n",
        "#loss_function = Loss_CategoricalCrossentropy()\n",
        "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "\n",
        "# Make a forward pass of our training data through this layer\n",
        "dense1.forward(X)\n",
        "\n",
        "# Forward pass through activation function\n",
        "# takes in output from previous layer\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "# Make a forward pass through second Dense layer\n",
        "# it takes output of activation function of first layer as intputs\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "# Make a forward pass through activation function\n",
        "# it takes output of second Dense layer here\n",
        "#activation2.forward(dense2.output)\n",
        "\n",
        "#print(dense1.output[:5])\n",
        "#print(activation2.output[:5])\n",
        "\n",
        "# Perform a forward pass through activation function\n",
        "# it takes the output of second dense layer here and returns loss\n",
        "#loss = loss_function.calculate(activation2.output, y)\n",
        "\n",
        "loss = loss_activation.forward(dense2.output, y)\n",
        "print(loss_activation.output[:5])\n",
        "\n",
        "print('loss: ', loss)\n",
        "\n",
        "# Calculate accuracy from output of activation2 and targets\n",
        "# calculate values along the first axis\n",
        "#predictions = np.argmax(activation2.output, axis=1)\n",
        "predictions = np.argmax(loss_activation.output, axis=1)\n",
        "if len(y.shape) == 2:\n",
        "  y = np.argmax(y, axis=1)\n",
        "accuracy = np.mean(predictions == y)\n",
        "\n",
        "# Print accuracy\n",
        "print('acc: ', accuracy)\n",
        "\n",
        "# Backward pass\n",
        "loss_activation.backward(loss_activation.output, y)\n",
        "dense2.backward(loss_activation.dinputs)\n",
        "activation1.backward(dense2.dinputs)\n",
        "dense1.backward(activation1.dinputs)\n",
        "\n",
        "# Print graidents\n",
        "print(dense1.dweights)\n",
        "print(dense1.dbiases)\n",
        "print(dense2.dweights)\n",
        "print(dense2.dbiases)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "TudIK7ORvRSm",
        "outputId": "3e19e7d6-a67a-4190-d1c9-b7a432519eae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n# 기존코드\\n#dataset 가지고 오기\\nX, y = spiral_data(samples=100, classes=3)\\n\\n# Dense layer 만들기 2 input features and 3 output values\\ndense1 = Layer_Dense(2,3)\\n\\n# ReLU Activation 만들기\\nactivation1 = Activation_ReLU()\\n\\n# Create second Dense Layer with 3 input features (as we take output of previous layer here) and 3 output values\\ndense2 = Layer_Dense(3,3)\\n\\n# Create Softmax activation (to be used with Dense Layer)\\n#activation2 = Activation_Softmax()\\n\\n# Create loss function\\n#loss_function = Loss_CategoricalCrossentropy()\\nloss_activation = Activation_Softmax_Loss_CategoricalCrossentropy()\\n\\n# Make a forward pass of our training data through this layer\\ndense1.forward(X)\\n\\n# Forward pass through activation function\\n# takes in output from previous layer\\nactivation1.forward(dense1.output)\\n\\n# Make a forward pass through second Dense layer\\n# it takes output of activation function of first layer as intputs\\ndense2.forward(activation1.output)\\n\\n# Make a forward pass through activation function\\n# it takes output of second Dense layer here\\n#activation2.forward(dense2.output)\\n\\n#print(dense1.output[:5])\\n#print(activation2.output[:5])\\n\\n# Perform a forward pass through activation function\\n# it takes the output of second dense layer here and returns loss\\n#loss = loss_function.calculate(activation2.output, y)\\n\\nloss = loss_activation.forward(dense2.output, y)\\nprint(loss_activation.output[:5])\\n\\nprint('loss: ', loss)\\n\\n# Calculate accuracy from output of activation2 and targets\\n# calculate values along the first axis\\n#predictions = np.argmax(activation2.output, axis=1)\\npredictions = np.argmax(loss_activation.output, axis=1)\\nif len(y.shape) == 2:\\n  y = np.argmax(y, axis=1)\\naccuracy = np.mean(predictions == y)\\n\\n# Print accuracy\\nprint('acc: ', accuracy)\\n\\n# Backward pass\\nloss_activation.backward(loss_activation.output, y)\\ndense2.backward(loss_activation.dinputs)\\nactivation1.backward(dense2.dinputs)\\ndense1.backward(activation1.dinputs)\\n\\n# Print graidents\\nprint(dense1.dweights)\\nprint(dense1.dbiases)\\nprint(dense2.dweights)\\nprint(dense2.dbiases)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the new sombined loss and activation class\n",
        "'''\n",
        "softmax_outputs = np.array([[0.7, 0.1, 0.2],\n",
        "                            [0.1, 0.5, 0.4],\n",
        "                            [0.02, 0.9, 0.08]])\n",
        "\n",
        "\n",
        "class_targets = np.array([0,1,1])\n",
        "\n",
        "softmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\n",
        "softmax_loss.backward(softmax_outputs, class_targets)\n",
        "dvalues1 = softmax_loss.dinputs\n",
        "\n",
        "activation = Activation_Softmax()\n",
        "activation.output = softmax_outputs\n",
        "loss = Loss_CategoricalCrossentropy()\n",
        "loss.backward(softmax_outputs, class_targets)\n",
        "activation.backward(loss.dinputs)\n",
        "dvalues2 = activation.dinputs\n",
        "\n",
        "print('Gradients: combined loss and activation')\n",
        "print(dvalues1)\n",
        "print('Gradients: seperate loss and activation')\n",
        "print(dvalues2)\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "O99liAGmdcfM",
        "outputId": "b75fac03-a4ab-4318-aa96-e0c6fb73964d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nsoftmax_outputs = np.array([[0.7, 0.1, 0.2],\\n                            [0.1, 0.5, 0.4],\\n                            [0.02, 0.9, 0.08]])\\n\\n\\nclass_targets = np.array([0,1,1])\\n\\nsoftmax_loss = Activation_Softmax_Loss_CategoricalCrossentropy()\\nsoftmax_loss.backward(softmax_outputs, class_targets)\\ndvalues1 = softmax_loss.dinputs\\n\\nactivation = Activation_Softmax()\\nactivation.output = softmax_outputs\\nloss = Loss_CategoricalCrossentropy()\\nloss.backward(softmax_outputs, class_targets)\\nactivation.backward(loss.dinputs)\\ndvalues2 = activation.dinputs\\n\\nprint('Gradients: combined loss and activation')\\nprint(dvalues1)\\nprint('Gradients: seperate loss and activation')\\nprint(dvalues2)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}