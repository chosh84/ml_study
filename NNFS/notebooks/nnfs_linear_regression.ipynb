{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nnfs_linear_regression",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNAp07/6fjQOjp15lVrzNYC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chosh84/ml_study/blob/main/NNFS/notebooks/nnfs_linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ag5xegBdoj0z",
        "outputId": "653fa114-0301-4801-d21d-bc95c7242fef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nnfs in /usr/local/lib/python3.7/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nnfs) (1.21.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import sine_data\n",
        "\n",
        "nnfs.init()"
      ],
      "metadata": {
        "id": "SzaKjxOjkNLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# neuron class 생성\n",
        "\n",
        "class Layer_Dense:\n",
        "\n",
        "  # Layer initialization\n",
        "  def __init__(self, n_inputs, n_neurons,\n",
        "               weight_regularizer_l1=0, weight_regularizer_l2=0,\n",
        "               bias_regularizer_l1=0, bias_regularizer_l2=0):\n",
        "    #Initialize weights and biases\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "    # Set regularization strength\n",
        "    self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "    self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "    self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "    self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # Remembering inputs for calculating backward pass of weights\n",
        "    self.inputs = inputs\n",
        "    # Calculate output values from inputs, weights and biases\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "  \n",
        "  #backward pass\n",
        "  def backward(self, dvalues):\n",
        "    # Gradients on parameters\n",
        "    self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "    self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "    \n",
        "    # Gradients on regularization\n",
        "    # L1 on weights\n",
        "    if self.weight_regularizer_l1 > 0:\n",
        "      dL1 = np.ones_like(self.weights)\n",
        "      dL1[self.weights < 0] = -1\n",
        "      self.dweights += self.weight_regularizer_l1 * dL1\n",
        "    \n",
        "    # L1 on biases\n",
        "    if self.bias_regularizer_l1 > 0:\n",
        "      dL1 = np.ones_like(self.biases)\n",
        "      dL1[self.biases < 0] = -1\n",
        "      self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "\n",
        "    # L2 on weights\n",
        "    if self.weight_regularizer_l2 > 0:\n",
        "      self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
        "    \n",
        "    # L2 on biases\n",
        "    if self.bias_regularizer_l2 > 0:\n",
        "      self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
        "    \n",
        "    # Gradients on values\n",
        "    self.dinputs = np.dot(dvalues, self.weights.T)"
      ],
      "metadata": {
        "id": "6jcspCUukRZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropout\n",
        "class Layer_Dropout:\n",
        "  # Init\n",
        "  def __init__(self, rate):\n",
        "    # Store rate, we invert it as for example for dropout # of 0.1 we need success rate of 0.9\n",
        "    self.rate = 1 - rate\n",
        "  \n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Save input values\n",
        "    self.inputs = inputs\n",
        "    # Generate and save scaled mask\n",
        "    self.binary_mask = np.random.binomial(1, self.rate, size=inputs.shape) / self.rate\n",
        "    # Apply mask to output values\n",
        "    self.output = inputs * self.binary_mask\n",
        "  \n",
        "  # Backward pass\n",
        "  def backward(self, dvalues):\n",
        "    # Gradient on values\n",
        "    self.dinputs = dvalues * self.binary_mask"
      ],
      "metadata": {
        "id": "yWhaoczJFlqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ReLU Activiation class 생성\n",
        "\n",
        "class Activation_ReLU:\n",
        "\n",
        "  #Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Remembering inputs for calculating backward pass of weights\n",
        "    self.inputs = inputs\n",
        "    # Calculate output values from input\n",
        "    self.output = np.maximum(0, inputs)\n",
        "\n",
        "  # Backward pass\n",
        "  def backward(self, dvalues):\n",
        "    # Since we need to modify the original variables,\n",
        "    # let's make a copy of the values first\n",
        "    self.dinputs = dvalues.copy()\n",
        "\n",
        "    # Zero gradient where input values are negetive\n",
        "    self.dinputs[self.inputs <= 0] = 0"
      ],
      "metadata": {
        "id": "QMw_kvyRScmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Activation class 생성\n",
        "# output 간의 확율로 변환 (총합이 1인 확율로 변환)\n",
        "\n",
        "class Activation_Softmax:\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Get unnormalized probabilities\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "\n",
        "    # Normalize them for each sample\n",
        "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "\n",
        "    self.output = probabilities\n",
        "\n",
        "  # Backward pass\n",
        "  def backward(self, dvalues):\n",
        "    # Create uninitialized array\n",
        "    self.dinputs = np.empty_like(dvalues)\n",
        "    # Enumerate outputs and gradients\n",
        "    for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
        "      # Flatten output array\n",
        "      single_output = single_output.reshape(-1, 1)\n",
        "      # Calculate Jacobian matrix of the output\n",
        "      jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "      # Calculate sample-wise gradient\n",
        "      # and add it to the array of sample gradients\n",
        "      self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
        "\n"
      ],
      "metadata": {
        "id": "kqjvNmuTp3eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sigmoid activation\n",
        "class Activation_Sigmoid:\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Save input and calculate/save output\n",
        "    # of the sigmoid function\n",
        "    self.inputs = inputs\n",
        "    self.output = 1 / (1 + np.exp(-inputs))\n",
        "\n",
        "  # Backward pass\n",
        "  def backward(self, dvalues):\n",
        "    # Derivatives - calculates from output of sigmoid function\n",
        "    self.dinputs = dvalues * (1 - self.output) * self.output"
      ],
      "metadata": {
        "id": "BuTAqLQa0FRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Activation\n",
        "class Activation_Linear:\n",
        "  # Forward pass\n",
        "  def forward(self, inputs):\n",
        "    # Just rememer the values\n",
        "    self.inputs = inputs\n",
        "    self.output = inputs\n",
        "\n",
        "  # Backward pass\n",
        "  def backward(self, dvalues):\n",
        "    # derivative is 1, 1 * dvalues = dvalues - the chain rule\n",
        "    self.dinputs = dvalues.copy()\n"
      ],
      "metadata": {
        "id": "SvaxTFhmN2UH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Common loss class\n",
        "class Loss:\n",
        "\n",
        "  # Calulates the data and regularization losses\n",
        "  # given model output and ground truth values\n",
        "  def calculate(self, output, y):\n",
        "\n",
        "    # Calculate sample losses\n",
        "    sample_losses = self.forward(output, y)\n",
        "\n",
        "    # Calculate mean loss\n",
        "    data_loss = np.mean(sample_losses)\n",
        "\n",
        "    # Return loss\n",
        "    return data_loss\n",
        "\n",
        "  # Regularization loss calculation\n",
        "  def regularization_loss(self, layer):\n",
        "    # 0 by default\n",
        "    regularization_loss = 0\n",
        "\n",
        "    # L1 regularization - weights\n",
        "    # calculate only when factor greater than 0\n",
        "    if layer.weight_regularizer_l1 > 0:\n",
        "      regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
        "    \n",
        "    # L2 regularization - weights\n",
        "    if layer.weight_regularizer_l2 > 0:\n",
        "      regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights * layer.weights)\n",
        "\n",
        "    # L1 regularization - bias\n",
        "    # calculate only when factor greater than 0\n",
        "    if layer.bias_regularizer_l1 > 0:\n",
        "      regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
        "    \n",
        "    # L2 regularization - bias\n",
        "    if layer.bias_regularizer_l2 > 0:\n",
        "      regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases)\n",
        "\n",
        "    return regularization_loss"
      ],
      "metadata": {
        "id": "m0ohO1sH2oe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross-entropy loss\n",
        "\n",
        "class Loss_CategoricalCrossentropy(Loss):\n",
        "\n",
        "  # Forward pass\n",
        "  def forward(self, y_pred, y_true):\n",
        "\n",
        "    # Number of samples in a batch\n",
        "    samples = len(y_pred)\n",
        "\n",
        "    # Clip data to prevent division by 0\n",
        "    # Clip both sides to not drag mean towards any value\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    # Probabilities of target values\n",
        "    # only if categorical labels\n",
        "    if len(y_true.shape) == 1:\n",
        "      correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "    # Mask values - only for one-hot encoded labels\n",
        "    elif len(y_true.shape) == 2:\n",
        "      correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
        "\n",
        "    # Losses\n",
        "    negative_log_likelihoods = -np.log(correct_confidences)\n",
        "    return negative_log_likelihoods\n",
        "\n",
        "  # Backward pass\n",
        "  def backward(self, dvalues, y_true):\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "    # Number of lables in every sample\n",
        "    # We'll use the first sample to count them\n",
        "    labels = len(dvalues[0])\n",
        "    # If lables are sparse, turn them into one-hot vector\n",
        "    if len(y_true.shape) == 1:\n",
        "      y_true = np.eye(labels)[y_true]\n",
        "    # Calculate gradient\n",
        "    self.dinputs = - y_true /dvalues\n",
        "    # Normalize gradient\n",
        "    self.dinputs = self.dinputs / samples\n",
        "    print(self.dinputs)"
      ],
      "metadata": {
        "id": "J41Squ1F3RGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax Classifier - combined Softmax activation\n",
        "# and cross-entropy loss for faster backward step\n",
        "class Activation_Softmax_Loss_CategoricalCrossentropy():\n",
        "  # Creates activation and loss function objects\n",
        "  def __init__(self):\n",
        "    self.activation = Activation_Softmax()\n",
        "    self.loss = Loss_CategoricalCrossentropy()\n",
        "  \n",
        "  # Forward pass\n",
        "  def forward(self, inputs, y_true):\n",
        "    # Output layer's activation function\n",
        "    self.activation.forward(inputs)\n",
        "    # Set the output\n",
        "    self.output = self.activation.output\n",
        "    # Calculate and return loss value\n",
        "    return self.loss.calculate(self.output, y_true)\n",
        "\n",
        "  # Backward pass\n",
        "  def backward(self, dvalues, y_true):\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "    # If labels are one-hot encoded,\n",
        "    # turn them in to discrete values\n",
        "    if len(y_true.shape)==2:\n",
        "      y_true = np.argmax(y_true, axis=1)\n",
        "    # Copy so we can safely modify\n",
        "    self.dinputs = dvalues.copy()\n",
        "    # Calculate gradient\n",
        "    self.dinputs[range(samples),y_true] -= 1\n",
        "    # Normalize gradient\n",
        "    self.dinputs = self.dinputs / samples\n",
        "    "
      ],
      "metadata": {
        "id": "UlUtg3G6bGtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Binary cross-entropy loss\n",
        "class Loss_BinaryCrossentropy(Loss):\n",
        "  # Forward pass\n",
        "  def forward(self, y_pred, y_true):\n",
        "    # Clip data to prevent division by 0 \n",
        "    # Clip both sides to not drag mean torwards any value\n",
        "    y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    # Calculate sample-wise loss\n",
        "    sample_losses = -(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
        "    sample_losses = np.mean(sample_losses, axis=1)\n",
        "\n",
        "    # Return losses\n",
        "    return sample_losses\n",
        "  \n",
        "  # Backward pass\n",
        "  def backward(self, dvalues, y_true):\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "    # Number of outputs in every sample\n",
        "    # We'll use the first sample to cout them\n",
        "    outputs = len(dvalues[0])\n",
        "\n",
        "    # Clip data to prevent division by 0 \n",
        "    # Clip both sides not to drag mean towards any value\n",
        "    clipped_dvalues = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    # Calculate gradient\n",
        "    self.dinputs = -(y_true / clipped_dvalues - (1 - y_true) / (1 - clipped_dvalues)) / outputs\n",
        "    # Normalize gradient\n",
        "    self.dinputs = self.dinputs / samples\n"
      ],
      "metadata": {
        "id": "ml7aoywn1aqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Squared Error loss\n",
        "class Loss_MeanSquaredError(Loss): # L2 loss\n",
        "  # Forward pass\n",
        "  def forward(self, y_pred, y_true):\n",
        "    # Calculate loss\n",
        "    sample_losses = np.mean((y_true - y_pred)**2, axis=-1)\n",
        "\n",
        "    # Return sample losses\n",
        "    return sample_losses\n",
        "\n",
        "  # Backward pass\n",
        "  def backward(self, dvalues, y_true):\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "    # Number of outputs in every sample\n",
        "    # We'll use the first sample to count them\n",
        "    outputs = len(dvalues[0])\n",
        "\n",
        "    # Gradient on values\n",
        "    self.dinputs = -2 * (y_true - dvalues) / outputs\n",
        "    # Normalize gradient\n",
        "    self.dinputs = self.dinputs / samples\n"
      ],
      "metadata": {
        "id": "tPRD2mh7PBr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mean Absolute Error loss\n",
        "class Loss_MeanAbsoluteError(Loss): # L1 loss\n",
        "  # Forward pass\n",
        "  def forward(self, y_pred, y_true):\n",
        "    # Calculate loss\n",
        "    sample_losses = np.mean(np.abs(y_true - y_pred), axis=-1)\n",
        "\n",
        "    # Return losses\n",
        "    return sample_losses\n",
        "  \n",
        "  # Backward pass\n",
        "  def backward(self, dvalues, y_true):\n",
        "    # Number of samples\n",
        "    samples = len(dvalues)\n",
        "    # Number of outputs in every sample\n",
        "    # We'll use the first sample to count them\n",
        "    outputs = len(dvalues[0])\n",
        "\n",
        "    # Calculate gradient\n",
        "    self.dinputs = np.sign(y_true - dvalues) / outputs\n",
        "    # Normalize gradient\n",
        "    self.dinputs = self.dinputs / samples\n"
      ],
      "metadata": {
        "id": "HdGU5Gh6Vb2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD optimizer\n",
        "class Optimizer_SGD:\n",
        "\n",
        "  # Initialize optimizer - set settings, \n",
        "  # learning rate of 1. is default for this optimizer\n",
        "  def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.momentum = momentum\n",
        "  \n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "  # Update parameters\n",
        "  def update_params(self, layer):\n",
        "    \n",
        "    # If we use momentum\n",
        "    if self.momentum:\n",
        "      # If layer does not contain momentum arrays, create them\n",
        "      # filled with zeros\n",
        "      if not hasattr(layer, 'weight_momentums'):\n",
        "        layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "        # If there is no momentum array for weights\n",
        "        # The array doesn't exists for biases yet either\n",
        "        layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "      \n",
        "      # Build weight updates  with momentum - take previous\n",
        "      # update multiplied by retain factor and update with \n",
        "      # current gradients\n",
        "      weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
        "      layer.weight_momentums = weight_updates\n",
        "\n",
        "      # Build biases updates\n",
        "      bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
        "      layer.bias_momentums = bias_updates\n",
        "\n",
        "    # Vanilla SGD updates (as before momentum update)\n",
        "    else:\n",
        "      weight_updates = -self.current_learning_rate * layer.dweights\n",
        "      bias_updates = -self.current_learning_rate * layer.dbiases\n",
        "\n",
        "    # Update weights and biases using either\n",
        "    # vanilla or momentum updates\n",
        "    layer.weights += weight_updates\n",
        "    layer.biases += bias_updates\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1"
      ],
      "metadata": {
        "id": "pQsCrT3n134x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adagrad optimizer\n",
        "class Optimizer_Adagrad:\n",
        "\n",
        "  # Initialize optimizer - set settings, \n",
        "  def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "  \n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "  # Update parameters\n",
        "  def update_params(self, layer):\n",
        "    # If layer does not contain cache arrays, create them\n",
        "    # filled with zeros\n",
        "    if not hasattr(layer, 'weight_cache'):\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "    \n",
        "    # Update cach with squared current gradients\n",
        "    layer.weight_cache += layer.dweights**2\n",
        "    layer.bias_cache += layer.dbiases**2\n",
        "\n",
        "    # Vanilla SGD parameter update + normalization \n",
        "    # with square rooted cache\n",
        "    layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "    layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1"
      ],
      "metadata": {
        "id": "b5J2TjqVjPBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# RMSprop optimizer\n",
        "class Optimizer_RMSprop:\n",
        "  # Initialize optimizer - set settings\n",
        "  def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, rho=0.9):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.rho = rho\n",
        "  \n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params(self): \n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "  \n",
        "  # Update parameters\n",
        "  def update_params(self, layer):\n",
        "    # If layer does not contain cache arrays,\n",
        "    # create them filled with zeros\n",
        "    if not hasattr(layer, 'weight_cache'):\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "    \n",
        "    # Update cache with squared current gradients\n",
        "    layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
        "    layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
        "\n",
        "    # Vanilla SGD parameter update + normalization \n",
        "    # with square rooted cache\n",
        "    layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "    layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "  \n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1"
      ],
      "metadata": {
        "id": "R8ymIskE5dZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adam optimizer\n",
        "class Optimizer_Adam:\n",
        "  # Initialize optimizer - set settings\n",
        "  def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "  \n",
        "  # Call once before any parameter updates\n",
        "  def pre_update_params(self): \n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "  \n",
        "  # Update parameters\n",
        "  def update_params(self, layer):\n",
        "    # If layer does not contain cache arrays,\n",
        "    # create them filled with zeros\n",
        "    if not hasattr(layer, 'weight_cache'):\n",
        "      layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "    \n",
        "    # Update momentum with current gradients\n",
        "    layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
        "    layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
        "\n",
        "    # Get corrected momentum \n",
        "    # self.iteration is 0 at first pass\n",
        "    # and we need to start with 1 here\n",
        "    weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "    bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "\n",
        "    # Update cache with the squared current gradients\n",
        "    layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
        "    layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
        "\n",
        "    # Get corrected cache\n",
        "    weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "    bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "    \n",
        "    # Vanilla SGD parameter update + normalization \n",
        "    # with square rooted cache\n",
        "    layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "    layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "    \n",
        "  \n",
        "  # Call once after any parameter updates\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1"
      ],
      "metadata": {
        "id": "vqr2V0mxC_zt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataset\n",
        "X, y = sine_data()\n",
        "\n",
        "# Create Dense layer with 1 input feature and 64 output values\n",
        "dense1 = Layer_Dense(1, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "# Create Dense layer with 1 input feature and 64 output values\n",
        "dense2 = Layer_Dense(64, 64)\n",
        "\n",
        "# Create ReLU activation (to be used with Dense layer):\n",
        "activation2 = Activation_ReLU()\n",
        "\n",
        "# Create second Dense layer with 64 input features (as we take output\n",
        "# of previous layer here) and 1 output value\n",
        "dense3 = Layer_Dense(64, 1)\n",
        "\n",
        "# Create Linear activation:\n",
        "activation3 = Activation_Linear()\n",
        "\n",
        "# Create loss function\n",
        "loss_function = Loss_MeanSquaredError()\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = Optimizer_Adam(learning_rate=0.005, decay=1e-3)\n",
        "\n",
        "# Accuracy precision for accuracy calculation\n",
        "# There are no really accuracy factor for regression problem,\n",
        "# but we can simulate/approximate it. We'll calculate it by checking\n",
        "# how many values have a difference to their ground truth equivalent\n",
        "# less than given precision\n",
        "# We'll calculate this precision as a fraction of standard deviation\n",
        "# of al the ground truth values\n",
        "accuracy_precision = np.std(y) / 250\n",
        "\n",
        "# Train in loop\n",
        "for epoch in range(10001):\n",
        "  # Make a forward pass of our training data through this layer\n",
        "  dense1.forward(X)\n",
        "\n",
        "  # Forward pass through activation function\n",
        "  # takes in output from previous layer\n",
        "  activation1.forward(dense1.output)\n",
        "\n",
        "  # Make a forward pass through second Dense layer\n",
        "  # it takes output of activation function of first layer as intputs\n",
        "  dense2.forward(activation1.output)\n",
        "\n",
        "  # Perform a forward pass through activation function\n",
        "  # takes the output of second dense layer here\n",
        "  activation2.forward(dense2.output)\n",
        "\n",
        "  # Make a forward pass through third Dense layer\n",
        "  # it takes output of activation function of second layer as intputs\n",
        "  dense3.forward(activation2.output)\n",
        "\n",
        "  # Perform a forward pass through activation function\n",
        "  # takes the output of third dense layer here\n",
        "  activation3.forward(dense3.output)\n",
        "\n",
        "  # Calculate loss from output of activation2 so Sigmoid activation\n",
        "  data_loss = loss_function.calculate(activation3.output, y)\n",
        "\n",
        "  # Calculate regularization penalty\n",
        "  regularization_loss = loss_function.regularization_loss(dense1) + \\\n",
        "                        loss_function.regularization_loss(dense2) + \\\n",
        "                        loss_function.regularization_loss(dense3)\n",
        "\n",
        "  # Calculate overall loss\n",
        "  loss = data_loss + regularization_loss\n",
        "\n",
        "  # Calculate accuracy from output of activation2 and targets\n",
        "  # To calculate it we're taking absolute difference between\n",
        "  # predictions and ground truth values and compare if differences\n",
        "  # are lower than given precision value\n",
        "  predictions = activation3.output\n",
        "  accuracy = np.mean(np.absolute(predictions - y) <\n",
        "                    accuracy_precision)\n",
        "  # Print accuracy\n",
        "  if not epoch % 100:\n",
        "    print(f'epoch: {epoch}, ' + f'acc: {accuracy:.3f},  ' + \n",
        "          f'loss: {loss:.3f}, (' + \n",
        "          f'data_loss: {data_loss:.3f}, ' + \n",
        "          f'reg_loss: {regularization_loss:.3f}), ' + \n",
        "          f'lr: {optimizer.current_learning_rate}')\n",
        "\n",
        "  # Backward pass\n",
        "  loss_function.backward(activation3.output, y)\n",
        "  activation3.backward(loss_function.dinputs)\n",
        "  dense3.backward(activation3.dinputs)\n",
        "  activation2.backward(dense3.dinputs)\n",
        "  dense2.backward(activation2.dinputs)\n",
        "  activation1.backward(dense2.dinputs)\n",
        "  dense1.backward(activation1.dinputs)\n",
        "\n",
        "  # Update weights and biases\n",
        "  optimizer.pre_update_params()\n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)\n",
        "  optimizer.update_params(dense3)\n",
        "  optimizer.post_update_params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OzNQoA44PzZ",
        "outputId": "62e7b97c-894b-4154-98fb-613928a7ea64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.002,  loss: 0.500, (data_loss: 0.500, reg_loss: 0.000), lr: 0.005\n",
            "epoch: 100, acc: 0.007,  loss: 0.084, (data_loss: 0.084, reg_loss: 0.000), lr: 0.004549590536851684\n",
            "epoch: 200, acc: 0.033,  loss: 0.034, (data_loss: 0.034, reg_loss: 0.000), lr: 0.004170141784820684\n",
            "epoch: 300, acc: 0.020,  loss: 0.003, (data_loss: 0.003, reg_loss: 0.000), lr: 0.003849114703618168\n",
            "epoch: 400, acc: 0.618,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0035739814152966403\n",
            "epoch: 500, acc: 0.605,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.00333555703802535\n",
            "epoch: 600, acc: 0.732,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0031269543464665416\n",
            "epoch: 700, acc: 0.763,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.002942907592701589\n",
            "epoch: 800, acc: 0.778,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0027793218454697055\n",
            "epoch: 900, acc: 0.789,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0026329647182727752\n",
            "epoch: 1000, acc: 0.135,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.002501250625312656\n",
            "epoch: 1100, acc: 0.825,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0023820867079561697\n",
            "epoch: 1200, acc: 0.837,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.002273760800363802\n",
            "epoch: 1300, acc: 0.453,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.002174858634188778\n",
            "epoch: 1400, acc: 0.853,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0020842017507294707\n",
            "epoch: 1500, acc: 0.857,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0020008003201280513\n",
            "epoch: 1600, acc: 0.861,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.001923816852635629\n",
            "epoch: 1700, acc: 0.869,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.001852537977028529\n",
            "epoch: 1800, acc: 0.874,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0017863522686673815\n",
            "epoch: 1900, acc: 0.882,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0017247326664367024\n",
            "epoch: 2000, acc: 0.885,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0016672224074691564\n",
            "epoch: 2100, acc: 0.887,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0016134236850596968\n",
            "epoch: 2200, acc: 0.891,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0015629884338855893\n",
            "epoch: 2300, acc: 0.896,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0015156107911488332\n",
            "epoch: 2400, acc: 0.899,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0014710208884966167\n",
            "epoch: 2500, acc: 0.898,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0014289797084881396\n",
            "epoch: 2600, acc: 0.906,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.001389274798555154\n",
            "epoch: 2700, acc: 0.908,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0013517166801838335\n",
            "epoch: 2800, acc: 0.919,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0013161358252171624\n",
            "epoch: 2900, acc: 0.920,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0012823800974608873\n",
            "epoch: 3000, acc: 0.156,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0012503125781445363\n",
            "epoch: 3100, acc: 0.929,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0012198097096852891\n",
            "epoch: 3200, acc: 0.927,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011907597046915933\n",
            "epoch: 3300, acc: 0.190,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011630611770179114\n",
            "epoch: 3400, acc: 0.927,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011366219595362584\n",
            "epoch: 3500, acc: 0.928,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0011113580795732384\n",
            "epoch: 3600, acc: 0.915,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010871928680147858\n",
            "epoch: 3700, acc: 0.935,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010640561821664183\n",
            "epoch: 3800, acc: 0.933,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010418837257762034\n",
            "epoch: 3900, acc: 0.242,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010206164523372118\n",
            "epoch: 4000, acc: 0.935,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0010002000400080014\n",
            "epoch: 4100, acc: 0.934,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009805844283192783\n",
            "epoch: 4200, acc: 0.748,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009617234083477593\n",
            "epoch: 4300, acc: 0.935,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009435742592942063\n",
            "epoch: 4400, acc: 0.936,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009260974254491572\n",
            "epoch: 4500, acc: 0.930,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0009092562284051646\n",
            "epoch: 4600, acc: 0.937,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.000893016610108948\n",
            "epoch: 4700, acc: 0.463,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008773469029654326\n",
            "epoch: 4800, acc: 0.941,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.000862217623728229\n",
            "epoch: 4900, acc: 0.943,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008476012883539582\n",
            "epoch: 5000, acc: 0.877,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008334722453742291\n",
            "epoch: 5100, acc: 0.944,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008198065256599442\n",
            "epoch: 5200, acc: 0.944,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0008065817067268914\n",
            "epoch: 5300, acc: 0.463,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007937767899666614\n",
            "epoch: 5400, acc: 0.943,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007813720893889669\n",
            "epoch: 5500, acc: 0.945,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007693491306354824\n",
            "epoch: 5600, acc: 0.155,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007576905591756327\n",
            "epoch: 5700, acc: 0.948,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007463800567248844\n",
            "epoch: 5800, acc: 0.949,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007354022650389764\n",
            "epoch: 5900, acc: 0.950,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007247427163357008\n",
            "epoch: 6000, acc: 0.950,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.000714387769681383\n",
            "epoch: 6100, acc: 0.950,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0007043245527539089\n",
            "epoch: 6200, acc: 0.901,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006945409084595084\n",
            "epoch: 6300, acc: 0.952,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006850253459377996\n",
            "epoch: 6400, acc: 0.951,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006757669955399379\n",
            "epoch: 6500, acc: 0.953,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006667555674089878\n",
            "epoch: 6600, acc: 0.957,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006579813133307014\n",
            "epoch: 6700, acc: 0.953,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006494349915573451\n",
            "epoch: 6800, acc: 0.955,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006411078343377356\n",
            "epoch: 6900, acc: 0.958,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.00063299151791366\n",
            "epoch: 7000, acc: 0.957,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006250781347668457\n",
            "epoch: 7100, acc: 0.958,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006173601679219657\n",
            "epoch: 7200, acc: 0.744,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006098304671301379\n",
            "epoch: 7300, acc: 0.964,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006024822267743102\n",
            "epoch: 7400, acc: 0.957,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005953089653530181\n",
            "epoch: 7500, acc: 0.954,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.000588304506412519\n",
            "epoch: 7600, acc: 0.964,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005814629608093965\n",
            "epoch: 7700, acc: 0.966,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005747787101965744\n",
            "epoch: 7800, acc: 0.960,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005682463916354131\n",
            "epoch: 7900, acc: 0.966,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005618608832453085\n",
            "epoch: 8000, acc: 0.961,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.00055561729081009\n",
            "epoch: 8100, acc: 0.867,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005495109352676119\n",
            "epoch: 8200, acc: 0.964,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005435373410153278\n",
            "epoch: 8300, acc: 0.955,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005376922249704269\n",
            "epoch: 8400, acc: 0.968,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005319714863283328\n",
            "epoch: 8500, acc: 0.966,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005263711969681019\n",
            "epoch: 8600, acc: 0.966,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005208875924575476\n",
            "epoch: 8700, acc: 0.967,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005155170636148056\n",
            "epoch: 8800, acc: 0.972,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005102561485865905\n",
            "epoch: 8900, acc: 0.969,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005051015254066068\n",
            "epoch: 9000, acc: 0.967,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005000500050005\n",
            "epoch: 9100, acc: 0.968,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004950985246063966\n",
            "epoch: 9200, acc: 0.959,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004902441415825081\n",
            "epoch: 9300, acc: 0.971,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004854840275754928\n",
            "epoch: 9400, acc: 0.968,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004808154630252909\n",
            "epoch: 9500, acc: 0.964,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.00047623583198399844\n",
            "epoch: 9600, acc: 0.969,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.00047174261722804036\n",
            "epoch: 9700, acc: 0.969,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.00046733339564445275\n",
            "epoch: 9800, acc: 0.972,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.00046300583387350687\n",
            "epoch: 9900, acc: 0.964,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.00045875768419121016\n",
            "epoch: 10000, acc: 0.970,  loss: 0.000, (data_loss: 0.000, reg_loss: 0.000), lr: 0.00045458678061641964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validate the model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X_test, y_test = sine_data()\n",
        "\n",
        "dense1.forward(X_test)\n",
        "\n",
        "activation1.forward(dense1.output)\n",
        "\n",
        "dense2.forward(activation1.output)\n",
        "\n",
        "activation2.forward(dense2.output)\n",
        "\n",
        "dense3.forward(activation2.output)\n",
        "\n",
        "activation3.forward(dense3.output)\n",
        "\n",
        "plt.plot(X_test, y_test)\n",
        "plt.plot(X_test, activation3.output)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "THChgZAnZp_B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "92a52df2-036e-42b1-d20c-b63e8e676306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}